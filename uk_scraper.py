import os
import re
from datetime import datetime
from typing import List, Dict, Optional

import requests
from bs4 import BeautifulSoup

# =========================
# 0. Supabase ì„¤ì • (REST API)
# =========================

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SUPABASE_URL / SUPABASE_SERVICE_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

BASE_REST_URL = SUPABASE_URL.rstrip("/") + "/rest/v1"

BASE_HEADERS = {
    "apikey": SUPABASE_SERVICE_KEY,
    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
    "Content-Type": "application/json",
}

# =========================
# 1. ê³µí†µ ìœ í‹¸
# =========================

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )
}


def safe_int(value: Optional[str]) -> Optional[int]:
    """ìˆ«ìì²˜ëŸ¼ ë³´ì´ë©´ int, ì•„ë‹ˆë©´ None."""
    if not value:
        return None
    value = value.strip()
    if not value.isdigit():
        return None
    return int(value)


def extract_chart_date(raw_text: str) -> Optional[str]:
    """
    ì˜ˆì‹œ: '14 November 2025 - 20 November 2025'
    ì•ìª½ ë‚ ì§œë¥¼ chart_date ë¡œ ì‚¬ìš©.
    """
    m = re.search(r"(\d{1,2} \w+ \d{4})\s*-\s*(\d{1,2} \w+ \d{4})", raw_text)
    if not m:
        return None

    start_str = m.group(1)
    try:
        d = datetime.strptime(start_str, "%d %B %Y").date()
        return d.isoformat()
    except ValueError:
        return None


def parse_officialcharts_text(raw_text: str) -> List[Dict]:
    """
    Official Charts í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸(raw_text)ë¥¼ ë°›ì•„ì„œ
    rank / title / artist / LW / Peak / Weeks / chart_date ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.

    ğŸ‘‰ "Number 1" ê°™ì€ íŒ¨í„´ì— ì˜ì¡´í•˜ì§€ ì•Šê³ 
       "LW:", "Peak:", "Weeks:" ë¼ì¸ì„ ê¸°ì¤€ìœ¼ë¡œ íŒŒì‹±í•œë‹¤.
    """
    chart_date = extract_chart_date(raw_text)

    # íŠ¹ìˆ˜ ê³µë°±(NBSP) â†’ ì¼ë°˜ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”
    text = raw_text.replace("\xa0", " ")

    # ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ê³µë°± ì¤„ ì œê±°
    lines = [ln.strip() for ln in text.splitlines()]
    lines = [ln for ln in lines if ln]

    entries: List[Dict] = []
    rank_counter = 0
    n = len(lines)
    i = 0

    while i < n:
        line = lines[i]

        # 1) "LW:" ê°€ ë“¤ì–´ê°„ ì¤„ì„ ê³¡ ë¸”ë¡ì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
        if "LW:" not in line:
            i += 1
            continue

        rank_counter += 1  # 1, 2, 3, ... ìˆœì„œëŒ€ë¡œ ë­í¬ ë¶€ì—¬

        # --- ì œëª© / ì•„í‹°ìŠ¤íŠ¸: LW: ë¼ì¸ ë°”ë¡œ ìœ„ ë‘ ì¤„ ---
        j = i - 1
        artist = ""
        title = ""

        if j >= 0:
            artist = lines[j].strip()
            j -= 1
            if j >= 0:
                title = lines[j].strip()

        # --- LW ê°’ íŒŒì‹± ---
        # ì˜ˆ: "1. LW: 2," ë˜ëŠ” "1. LW: New"
        m_lw = re.search(r"LW\s*:\s*([0-9]+|New|RE)", line, re.IGNORECASE)
        if m_lw:
            lw_raw = m_lw.group(1).strip()
            # "New", "RE" ê°™ì€ í…ìŠ¤íŠ¸ëŠ” None ì²˜ë¦¬
            last_week_rank = safe_int(lw_raw)
        else:
            last_week_rank = None

        peak_rank = None
        weeks_on_chart = None

        # 2) ì•„ë˜ìª½ ì¤„ë“¤ì—ì„œ Peak / Weeks ê°’ ì°¾ê¸°
        k = i + 1
        while k < n and "LW:" not in lines[k]:
            if "Peak:" in lines[k]:
                m_peak = re.search(r"Peak\s*:\s*([0-9]+)", lines[k], re.IGNORECASE)
                if m_peak:
                    peak_rank = safe_int(m_peak.group(1))

            if "Weeks:" in lines[k]:
                m_weeks = re.search(r"Weeks\s*:\s*([0-9]+)", lines[k], re.IGNORECASE)
                if m_weeks:
                    weeks_on_chart = safe_int(m_weeks.group(1))

            # ë‹¤ìŒ ê³¡ ë¸”ë¡(LW:) ì„ ë§Œë‚˜ê¸° ì „ê¹Œì§€ëŠ” ê°™ì€ ê³¡ì˜ ì •ë³´ë¼ê³  ë³¸ë‹¤.
            k += 1

        entries.append(
            {
                "rank": rank_counter,
                "title": title,
                "artist": artist,
                "last_week_rank": last_week_rank,
                "peak_rank": peak_rank,
                "weeks_on_chart": weeks_on_chart,
                "chart_date": chart_date,
            }
        )

        # ë‹¤ìŒ íƒìƒ‰ ì‹œì‘ ìœ„ì¹˜ ê°±ì‹ 
        i = k

    print(f"[DEBUG] parsed entries ê°œìˆ˜: {len(entries)}")
    return entries


def fetch_official_chart(chart_path: str) -> List[Dict]:
    """
    chart_path ì˜ˆì‹œ:
      - 'singles-chart/'
      - 'albums-chart/'
    """
    url = f"https://www.officialcharts.com/charts/{chart_path}"
    print(f"[UK] ìš”ì²­ URL: {url}")

    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    raw_text = soup.get_text("\n", strip=True)

    entries = parse_officialcharts_text(raw_text)
    print(f"[UK] {chart_path} ì—ì„œ {len(entries)}ê°œ í•­ëª© íŒŒì‹±")
    return entries


# =========================
# 2. Supabase REST ì €ì¥
# =========================

def replace_entries_for_date(table_name: str, entries: List[Dict]) -> None:
    """ê°™ì€ chart_date ë°ì´í„° ì‹¹ ì§€ìš°ê³  ìƒˆë¡œ ë„£ê¸°."""
    if not entries:
        print(f"[WARN] {table_name}: ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    chart_date = entries[0]["chart_date"]
    if not chart_date:
        print(f"[WARN] {table_name}: chart_date ì—†ìŒ, ì €ì¥ ìŠ¤í‚µ.")
        return

    # 1) ê¸°ì¡´ í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì‚­ì œ
    delete_url = f"{BASE_REST_URL}/{table_name}?chart_date=eq.{chart_date}"
    print(f"[Supabase] {table_name} {chart_date} ë°ì´í„° ì‚­ì œ: {delete_url}")
    r_del = requests.delete(delete_url, headers=BASE_HEADERS, timeout=20)
    if not r_del.ok:
        print(f"[Supabase] {table_name} ì‚­ì œ ì‹¤íŒ¨: {r_del.status_code} {r_del.text}")

    # 2) ìƒˆ ë°ì´í„° insert
    insert_url = f"{BASE_REST_URL}/{table_name}"
    headers = {**BASE_HEADERS, "Prefer": "return=representation"}
    print(f"[Supabase] {table_name} {len(entries)}ê°œ í–‰ insert...")
    r_ins = requests.post(insert_url, headers=headers, json=entries, timeout=30)
    if not r_ins.ok:
        print(f"[ERROR] {table_name} insert ì‹¤íŒ¨: {r_ins.status_code} {r_ins.text}")
        r_ins.raise_for_status()
    else:
        print(f"[OK] {table_name} insert ì™„ë£Œ.")


# =========================
# 3. ì‹¤í–‰ íë¦„
# =========================

def update_uk_singles_chart():
    print("=== UK Official Singles Chart ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")
    entries = fetch_official_chart("singles-chart/")
    replace_entries_for_date("uk_singles_entries", entries)
    print("=== UK Official Singles Chart ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ ===\n")


def update_uk_albums_chart():
    print("=== UK Official Albums Chart ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")
    entries = fetch_official_chart("albums-chart/")
    replace_entries_for_date("uk_albums_entries", entries)
    print("=== UK Official Albums Chart ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ ===\n")


if __name__ == "__main__":
    try:
        update_uk_singles_chart()
        update_uk_albums_chart()
        print("ëª¨ë“  UK ì°¨íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ âœ…")
    except Exception:
        import traceback

        print("[FATAL] UK ì°¨íŠ¸ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:")
        traceback.print_exc()
        raise
