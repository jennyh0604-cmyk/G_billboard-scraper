import os
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from supabase import create_client

# ---------------------------------------------------
# Supabase ì„¤ì •
# ---------------------------------------------------
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SUPABASE_URL / SUPABASE_SERVICE_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

SINGLES_URL = "https://www.officialcharts.com/charts/singles-chart/"
ALBUMS_URL = "https://www.officialcharts.com/charts/albums-chart/"


# ---------------------------------------------------
# ê³µí†µ ìœ í‹¸
# ---------------------------------------------------
def parse_stat(text: str):
    """
    'LW: 2' / 'Last week: 2' / 'Weeks: 6' ê°™ì€ ë¬¸ìì—´ì—ì„œ
    ìˆ«ìë§Œ ë½‘ì•„ì„œ intë¡œ ë°˜í™˜. ìˆ«ìê°€ ì—†ìœ¼ë©´ None.
    """
    nums = re.findall(r"\d+", text)
    return int(nums[0]) if nums else None


# ---------------------------------------------------
# ë©”ì¸ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
# ---------------------------------------------------
def scrape_uk_chart(url: str, table: str):
    print(f"\n=== UK ì°¨íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")
    print(f"[URL] {url}")
    print(f"[TABLE] {table}\n")

    resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # ì°¨íŠ¸ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
    chart_date = datetime.utcnow().strftime("%Y-%m-%d")
    date_elem = soup.find(string=re.compile(r"\d{1,2}\s+\w+\s+\d{4}"))
    if date_elem:
        try:
            # ì˜ˆ: "22 November 2024" í˜•ì‹ íŒŒì‹±
            date_str = re.search(r"\d{1,2}\s+\w+\s+\d{4}", date_elem.string).group()
            chart_date = datetime.strptime(date_str, "%d %B %Y").strftime("%Y-%m-%d")
            print(f"[INFO] ì°¨íŠ¸ ë‚ ì§œ: {chart_date}")
        except:
            pass

    results = []

    # Official Chartsì˜ ì‹¤ì œ êµ¬ì¡°: ê° ì°¨íŠ¸ í•­ëª©ì´ íŠ¹ì • div/article ì•ˆì— ìˆìŒ
    # 'title' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œë“¤ì„ ì°¾ê±°ë‚˜, ì „ì²´ êµ¬ì¡°ë¥¼ íŒŒì•…
    
    # ë°©ë²• 1: table ë˜ëŠ” section ê¸°ë°˜ íŒŒì‹±
    chart_items = soup.select("table.chart-positions tr") or \
                  soup.select("div.chart-item") or \
                  soup.select("article.chart-item")
    
    if chart_items:
        print(f"[INFO] {len(chart_items)}ê°œ ì°¨íŠ¸ í•­ëª© ë°œê²¬ (êµ¬ì¡°í™”ëœ ë°©ì‹)")
        for item in chart_items:
            rank_elem = item.select_one(".position, .rank, [class*='position']")
            title_elem = item.select_one(".track, .title, a[href*='/search/']")
            artist_elem = item.select_one(".artist, a[href*='/artist/']")
            
            lw_elem = item.find(string=re.compile(r"Last\s+week|LW", re.I))
            peak_elem = item.find(string=re.compile(r"Peak", re.I))
            weeks_elem = item.find(string=re.compile(r"Weeks?\s+on\s+chart", re.I))
            
            rank = parse_stat(rank_elem.get_text()) if rank_elem else None
            title = title_elem.get_text(strip=True) if title_elem else "Unknown"
            artist = artist_elem.get_text(strip=True) if artist_elem else "Unknown"
            
            lw = parse_stat(lw_elem.find_next(string=re.compile(r"\d+"))) if lw_elem else None
            peak = parse_stat(peak_elem.find_next(string=re.compile(r"\d+"))) if peak_elem else None
            weeks = parse_stat(weeks_elem.find_next(string=re.compile(r"\d+"))) if weeks_elem else None
            
            results.append({
                "chart_date": chart_date,
                "rank": rank,
                "title": title,
                "artist": artist,
                "last_week_rank": lw,
                "peak_rank": peak,
                "weeks_on_chart": weeks,
            })
    
    # ë°©ë²• 2: ê¸°ì¡´ ë°©ì‹ ê°œì„  (fallback)
    else:
        print("[INFO] í´ë°± ë°©ì‹ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        number_tags = soup.find_all(string=re.compile(r"Number\s+\d+"))
        
        for num_tag in number_tags:
            m = re.search(r"\d+", str(num_tag))
            rank = int(m.group()) if m else None
            
            # ë¶€ëª¨ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ë” ì •í™•í•œ ë²”ìœ„ ì§€ì •)
            container = num_tag.find_parent(['div', 'section', 'article', 'tr', 'li'])
            if not container:
                container = num_tag.parent
            
            # ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œë§Œ ë§í¬ ê²€ìƒ‰
            title = "Unknown"
            artist = "Unknown"
            lw = peak = weeks = None
            
            # ì œëª©ê³¼ ì•„í‹°ìŠ¤íŠ¸ ì¶”ì¶œ ê°œì„ 
            links = container.find_all("a", href=True)
            track_links = []
            artist_links = []
            
            for a in links:
                href = a.get("href", "")
                txt = a.get_text(strip=True)
                
                if not txt or txt.startswith("Image:"):
                    continue
                
                # URL íŒ¨í„´ìœ¼ë¡œ êµ¬ë¶„
                if "/search/" in href or "/tracks/" in href:
                    track_links.append(txt)
                elif "/artist/" in href:
                    artist_links.append(txt)
                else:
                    # hrefê°€ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°, ìˆœì„œëŒ€ë¡œ ì¶”ê°€
                    if not track_links:
                        track_links.append(txt)
                    elif not artist_links:
                        artist_links.append(txt)
            
            if track_links:
                title = track_links[0]
            if artist_links:
                artist = artist_links[0]
            
            # í†µê³„ ì •ë³´ ì¶”ì¶œ (ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œë§Œ)
            stats_text = container.get_text()
            
            # Last Week
            lw_match = re.search(r"(?:Last\s+week|LW)[:\s]*(\d+)", stats_text, re.I)
            if lw_match:
                lw = int(lw_match.group(1))
            
            # Peak
            peak_match = re.search(r"Peak[:\s]*(\d+)", stats_text, re.I)
            if peak_match:
                peak = int(peak_match.group(1))
            
            # Weeks on chart
            weeks_match = re.search(r"Weeks?[:\s]*(\d+)", stats_text, re.I)
            if weeks_match:
                weeks = int(weeks_match.group(1))
            
            results.append({
                "chart_date": chart_date,
                "rank": rank,
                "title": title,
                "artist": artist,
                "last_week_rank": lw,
                "peak_rank": peak,
                "weeks_on_chart": weeks,
            })

    # ë¡œê·¸ ì¶œë ¥
    print(f"\n[ìˆ˜ì§‘ ê²°ê³¼ ìƒ˜í”Œ]")
    for item in results[:5]:
        print(f"#{item['rank']} - {item['title']} by {item['artist']}")
        print(f"  LW: {item['last_week_rank']}, Peak: {item['peak_rank']}, Weeks: {item['weeks_on_chart']}")

    # ---------------------------------------------------
    # Supabase ì—…ì„œíŠ¸
    # ---------------------------------------------------
    if results:
        print(f"\n{table} â†’ {len(results)}ê°œ í•­ëª© ì—…ì„œíŠ¸ ì¤‘â€¦")
        supabase.table(table).upsert(results).execute()
        print(f"{table} ì €ì¥ ì™„ë£Œ! âœ…\n")
    else:
        print(f"[WARN] {table}ì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ---------------------------------------------------
# main
# ---------------------------------------------------
def main():
    scrape_uk_chart(SINGLES_URL, "uk_singles_entries")
    scrape_uk_chart(ALBUMS_URL, "uk_albums_entries")
    print("ğŸ‰ ëª¨ë“  UK ì°¨íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
