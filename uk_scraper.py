import os
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from supabase import create_client

# ---------------------------------------------------
# Supabase ì„¤ì •
# ---------------------------------------------------
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ SUPABASE_URL / SUPABASE_SERVICE_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

SINGLES_URL = "https://www.officialcharts.com/charts/singles-chart/"
ALBUMS_URL = "https://www.officialcharts.com/charts/albums-chart/"


def parse_stat_value(text: str) -> int:
    """í†µê³„ ê°’ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
    if not text:
        return None
    text = text.strip()
    # "New", "RE" ê°™ì€ íŠ¹ìˆ˜ê°’ì€ None ì²˜ë¦¬
    if text.upper() in ["NEW", "RE", "-"]:
        return None
    # ìˆ«ìë§Œ ì¶”ì¶œ
    match = re.search(r"\d+", text)
    return int(match.group()) if match else None


def scrape_uk_chart(url: str, table: str):
    """UK Official Charts ìŠ¤í¬ë˜í•‘ - ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜"""
    print(f"\n{'='*70}")
    print(f"ğŸµ UK ì°¨íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    print(f"ğŸ“ URL: {url}")
    print(f"ğŸ’¾ TABLE: {table}")
    print(f"{'='*70}\n")

    # í˜ì´ì§€ ìš”ì²­
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    resp = requests.get(url, headers=headers, timeout=30)
    resp.raise_for_status()
    
    html_content = resp.text
    soup = BeautifulSoup(html_content, "html.parser")

    # ì°¨íŠ¸ ë‚ ì§œ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
    chart_date = datetime.utcnow().strftime("%Y-%m-%d")

    results = []
    
    # ì‹¤ì œ HTMLì—ì„œ íŒ¨í„´ ì°¾ê¸°:
    # 1) ì œëª© ë§í¬: /songs/xxx
    # 2) ì•„í‹°ìŠ¤íŠ¸ ë§í¬: /artist/xxx
    # 3) í†µê³„: "LW: 1," "Peak: 2," "Weeks: 3"
    
    # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ íŒ¨í„´ ë§¤ì¹­
    page_text = soup.get_text()
    
    # ê° ê³¡ë§ˆë‹¤ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´: [ê³¡ì œëª©] [ì•„í‹°ìŠ¤íŠ¸] LW: X, Peak: Y, Weeks: Z
    # ë˜ëŠ”: [ê³¡ì œëª©] [ì•„í‹°ìŠ¤íŠ¸] - LW: X, - Peak: Y, - Weeks: Z
    
    # ëª¨ë“  /songs/ ë§í¬ ì°¾ê¸° (ê³¡ ì œëª©)
    # SinglesëŠ” /songs/, AlbumsëŠ” /albums/ ì‚¬ìš©
    if "singles" in url.lower():
        content_links = soup.find_all("a", href=re.compile(r"/songs/"))
        artist_pattern = r"/artist/"
    else:
        content_links = soup.find_all("a", href=re.compile(r"/albums/"))
        artist_pattern = r"/artist/"
    
    print(f"ğŸ“Š ë°œê²¬ëœ í•­ëª© ë§í¬: {len(content_links)}ê°œ\n")
    
    for idx, song_link in enumerate(content_links, start=1):
        title = song_link.get_text(strip=True)
        if not title or title.startswith("Image:"):
            continue
            
        rank = idx
        artist = "Unknown"
        lw = peak = weeks = None
        
        # ê³¡ ë§í¬ ë‹¤ìŒì— ìˆëŠ” ì•„í‹°ìŠ¤íŠ¸ ë§í¬ ì°¾ê¸°
        next_sibling = song_link.find_next_sibling("a")
        if not next_sibling:
            # í˜•ì œê°€ ì—†ìœ¼ë©´ ë¶€ëª¨ì˜ ë‹¤ìŒ ë§í¬ ì°¾ê¸°
            parent = song_link.parent
            if parent:
                next_link = parent.find_next("a")
                if next_link and artist_pattern in next_link.get("href", ""):
                    artist = next_link.get_text(strip=True)
        elif artist_pattern in next_sibling.get("href", ""):
            artist = next_sibling.get_text(strip=True)
        
        # í†µê³„ ì •ë³´ ì¶”ì¶œ: í˜„ì¬ ë§í¬ë¶€í„° ë„“ì€ ë²”ìœ„ì—ì„œ ì°¾ê¸°
        search_start = html_content.find(title)
        if search_start != -1:
            # ì œëª© ìœ„ì¹˜ë¶€í„° 500ì ë²”ìœ„ì—ì„œ í†µê³„ ì°¾ê¸°
            search_text = html_content[search_start:search_start + 500]
            
            # ì‹¤ì œ HTML íŒ¨í„´: "LW: 1," ë˜ëŠ” "- LW: 1," ë˜ëŠ” "LW:1"
            # LW ì¶”ì¶œ - ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„
            lw_patterns = [
                r"LW[:\s]+(\d+)",  # LW: 1 ë˜ëŠ” LW:1
                r"Last\s+week[:\s]+(\d+)",  # Last week: 1
                r"-\s*LW[:\s]+(\d+)",  # - LW: 1
            ]
            for pattern in lw_patterns:
                lw_match = re.search(pattern, search_text, re.I)
                if lw_match:
                    lw = int(lw_match.group(1))
                    break
            
            # Newë‚˜ RE ì²˜ë¦¬
            if re.search(r"LW[:\s]+(New|RE)", search_text, re.I):
                lw = None
            
            # Peak ì¶”ì¶œ
            peak_patterns = [
                r"Peak[:\s]+(\d+)",
                r"-\s*Peak[:\s]+(\d+)",
            ]
            for pattern in peak_patterns:
                peak_match = re.search(pattern, search_text, re.I)
                if peak_match:
                    peak = int(peak_match.group(1))
                    break
            
            # Weeks ì¶”ì¶œ
            weeks_patterns = [
                r"Weeks[:\s]+(\d+)",
                r"-\s*Weeks[:\s]+(\d+)",
            ]
            for pattern in weeks_patterns:
                weeks_match = re.search(pattern, search_text, re.I)
                if weeks_match:
                    weeks = int(weeks_match.group(1))
                    break
        
        # ë°ì´í„° ì €ì¥
        entry = {
            "chart_date": chart_date,
            "rank": rank,
            "title": title,
            "artist": artist,
            "last_week_rank": lw,
            "peak_rank": peak,
            "weeks_on_chart": weeks,
        }
        results.append(entry)
        
        # ì²˜ìŒ 10ê°œ ì¶œë ¥
        if rank <= 10:
            print(f"#{rank:3d} | {title[:35]:<35} | {artist[:25]:<25}")
            print(f"       LW: {str(lw) if lw else 'New':>4} | Peak: {peak or '-':>3} | Weeks: {weeks or '-':>3}")
            print()
    
    # ê²°ê³¼ í™•ì¸
    print(f"\n{'='*70}")
    print(f"âœ… ì´ {len(results)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ")
    
    if not results:
        print("âš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print(f"{'='*70}\n")
        return
    
    # í†µê³„ ë°ì´í„° ìˆ˜ì§‘ë¥  í™•ì¸
    lw_count = sum(1 for r in results if r["last_week_rank"] is not None)
    peak_count = sum(1 for r in results if r["peak_rank"] is not None)
    weeks_count = sum(1 for r in results if r["weeks_on_chart"] is not None)
    
    print(f"ğŸ“ˆ ë°ì´í„° ìˆ˜ì§‘ë¥ :")
    print(f"   - Last Week: {lw_count}/{len(results)} ({lw_count/len(results)*100:.1f}%)")
    print(f"   - Peak: {peak_count}/{len(results)} ({peak_count/len(results)*100:.1f}%)")
    print(f"   - Weeks: {weeks_count}/{len(results)} ({weeks_count/len(results)*100:.1f}%)")
    print(f"{'='*70}\n")
    
    # Supabase ì—…ì„œíŠ¸
    print(f"ğŸ’¾ {table} í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
    try:
        supabase.table(table).upsert(results).execute()
        print(f"âœ… {table} ì €ì¥ ì™„ë£Œ!\n")
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        print(f"ìƒ˜í”Œ ë°ì´í„°: {results[0] if results else 'None'}")
        raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        scrape_uk_chart(SINGLES_URL, "uk_singles_entries")
        scrape_uk_chart(ALBUMS_URL, "uk_albums_entries")
        
        print(f"\n{'='*70}")
        print("ğŸ‰ ëª¨ë“  UK ì°¨íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        print(f"{'='*70}\n")
    except Exception as e:
        print(f"\nâŒì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
